{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# NIFTY50 Stock Predictor - Training on Google Colab\n",
        "\n",
        "This notebook trains the HybridForecaster model on all 50 NIFTY stocks.\n",
        "\n",
        "**Instructions:**\n",
        "1. Upload your `Nifty50-predictor` folder to Google Drive\n",
        "2. Make sure Runtime ‚Üí Change runtime type ‚Üí GPU is selected\n",
        "3. Run all cells in order\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "import math\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from tqdm import tqdm\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"CHECKING YOUR GOOGLE DRIVE STRUCTURE\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "drive_root = Path(\"/content/drive/MyDrive\")\n",
        "print(f\"\\nFiles/folders in your Google Drive root:\")\n",
        "for item in sorted(drive_root.iterdir()):\n",
        "    print(f\"  {'üìÅ' if item.is_dir() else 'üìÑ'} {item.name}\")\n",
        "\n",
        "DRIVE_PATH = Path(\"/content/drive/MyDrive/Nifty50-predictor\")\n",
        "\n",
        "if not DRIVE_PATH.exists():\n",
        "    print(f\"\\n‚ùå ERROR: Folder not found: {DRIVE_PATH}\")\n",
        "    print(\"\\nLooking for similar folders...\")\n",
        "    for item in drive_root.iterdir():\n",
        "        if \"nifty\" in item.name.lower() or \"predictor\" in item.name.lower():\n",
        "            print(f\"  Found: {item}\")\n",
        "    print(\"\\n‚ö†Ô∏è  Please upload your Nifty50-predictor folder to Google Drive root!\")\n",
        "    print(\"   Or update DRIVE_PATH below to match your folder name.\")\n",
        "else:\n",
        "    print(f\"\\n‚úÖ Project folder found: {DRIVE_PATH}\")\n",
        "    print(f\"\\nContents of project folder:\")\n",
        "    for item in sorted(DRIVE_PATH.iterdir()):\n",
        "        print(f\"  {'üìÅ' if item.is_dir() else 'üìÑ'} {item.name}\")\n",
        "\n",
        "PROCESSED_DIR = DRIVE_PATH / \"data\" / \"processed\"\n",
        "MODELS_DIR = DRIVE_PATH / \"models\"\n",
        "\n",
        "if PROCESSED_DIR.exists():\n",
        "    csv_files = list(PROCESSED_DIR.glob(\"*.csv\"))\n",
        "    print(f\"\\n‚úÖ Processed data folder found: {PROCESSED_DIR}\")\n",
        "    print(f\"   CSV files found: {len(csv_files)}\")\n",
        "    if csv_files:\n",
        "        print(f\"   First 5 files: {[f.name for f in csv_files[:5]]}\")\n",
        "else:\n",
        "    print(f\"\\n‚ùå ERROR: Processed data folder not found: {PROCESSED_DIR}\")\n",
        "    print(\"\\n‚ö†Ô∏è  You need to run download.py and feature_engineering.py first!\")\n",
        "    print(\"   Or upload the data/processed folder with CSV files.\")\n",
        "    \n",
        "    data_dir = DRIVE_PATH / \"data\"\n",
        "    if data_dir.exists():\n",
        "        print(f\"\\nContents of data folder:\")\n",
        "        for item in sorted(data_dir.iterdir()):\n",
        "            print(f\"  {'üìÅ' if item.is_dir() else 'üìÑ'} {item.name}\")\n",
        "\n",
        "SEQ_LENGTH = 120\n",
        "BATCH_SIZE = 64\n",
        "LEARNING_RATE = 1e-4\n",
        "EPOCHS = 50\n",
        "\n",
        "FEATURE_COLUMNS = [\"Open\", \"High\", \"Low\", \"Close\", \"Volume\"]\n",
        "TARGET_COLUMNS = [\"Return_3M\", \"Return_1Y\", \"Return_3Y\"]\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"\\n{'=' * 60}\")\n",
        "print(f\"Using device: {device}\")\n",
        "if device.type == 'cuda':\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "\n",
        "MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "print(f\"\\nProject path: {DRIVE_PATH}\")\n",
        "print(f\"Processed data: {PROCESSED_DIR}\")\n",
        "print(f\"Models will be saved to: {MODELS_DIR}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class StockDataset(Dataset):\n",
        "    def __init__(self, data_dir, seq_length=120, tickers=None, train=True, scaler_params=None):\n",
        "        self.data_dir = Path(data_dir)\n",
        "        self.seq_length = seq_length\n",
        "        self.train = train\n",
        "        \n",
        "        self.X = []\n",
        "        self.Y = []\n",
        "        self.scaler_params = scaler_params or {}\n",
        "        \n",
        "        if tickers:\n",
        "            csv_files = [self.data_dir / f\"{t}.csv\" for t in tickers]\n",
        "            csv_files = [f for f in csv_files if f.exists()]\n",
        "        else:\n",
        "            csv_files = list(self.data_dir.glob(\"*.csv\"))\n",
        "        \n",
        "        for csv_file in csv_files:\n",
        "            self._process_stock(csv_file)\n",
        "        \n",
        "        self.X = np.array(self.X, dtype=np.float32)\n",
        "        self.Y = np.array(self.Y, dtype=np.float32)\n",
        "        \n",
        "        self._normalize_data()\n",
        "    \n",
        "    def _process_stock(self, filepath):\n",
        "        df = pd.read_csv(filepath, index_col=0, parse_dates=True)\n",
        "        \n",
        "        features = df[FEATURE_COLUMNS].values\n",
        "        targets = df[TARGET_COLUMNS].values\n",
        "        \n",
        "        for i in range(len(df) - self.seq_length):\n",
        "            x = features[i:i + self.seq_length]\n",
        "            y = targets[i + self.seq_length - 1]\n",
        "            \n",
        "            if np.isnan(x).any() or np.isnan(y).any():\n",
        "                continue\n",
        "            \n",
        "            self.X.append(x)\n",
        "            self.Y.append(y)\n",
        "    \n",
        "    def _normalize_data(self):\n",
        "        if self.train:\n",
        "            X_flat = self.X.reshape(-1, len(FEATURE_COLUMNS))\n",
        "            self.scaler_params = {\n",
        "                'min': X_flat.min(axis=0),\n",
        "                'max': X_flat.max(axis=0),\n",
        "            }\n",
        "        \n",
        "        min_vals = self.scaler_params['min']\n",
        "        max_vals = self.scaler_params['max']\n",
        "        range_vals = max_vals - min_vals\n",
        "        range_vals[range_vals == 0] = 1\n",
        "        self.X = (self.X - min_vals) / range_vals\n",
        "    \n",
        "    def get_scaler_params(self):\n",
        "        return self.scaler_params\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return torch.tensor(self.X[idx]), torch.tensor(self.Y[idx])\n",
        "\n",
        "print(\"StockDataset class defined ‚úÖ\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        \n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        \n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        \n",
        "        self.register_buffer('pe', pe)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:, :x.size(1), :]\n",
        "        return self.dropout(x)\n",
        "\n",
        "\n",
        "class HybridForecaster(nn.Module):\n",
        "    def __init__(self, input_size=5, seq_length=120, cnn_channels=64, lstm_hidden=128,\n",
        "                 lstm_layers=2, transformer_heads=4, transformer_layers=2, dropout=0.2):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.cnn = nn.Sequential(\n",
        "            nn.Conv1d(input_size, cnn_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm1d(cnn_channels),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(cnn_channels, cnn_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm1d(cnn_channels),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "        \n",
        "        self.bilstm = nn.LSTM(\n",
        "            input_size=cnn_channels,\n",
        "            hidden_size=lstm_hidden,\n",
        "            num_layers=lstm_layers,\n",
        "            batch_first=True,\n",
        "            bidirectional=True,\n",
        "            dropout=dropout if lstm_layers > 1 else 0,\n",
        "        )\n",
        "        \n",
        "        bilstm_output_size = lstm_hidden * 2\n",
        "        \n",
        "        self.pos_encoder = PositionalEncoding(bilstm_output_size, seq_length, dropout)\n",
        "        \n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=bilstm_output_size,\n",
        "            nhead=transformer_heads,\n",
        "            dim_feedforward=bilstm_output_size * 4,\n",
        "            dropout=dropout,\n",
        "            batch_first=True,\n",
        "        )\n",
        "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=transformer_layers)\n",
        "        \n",
        "        self.feature_pool = nn.AdaptiveAvgPool1d(1)\n",
        "        \n",
        "        self.head_3m = nn.Sequential(\n",
        "            nn.Linear(bilstm_output_size, 64), nn.ReLU(), nn.Dropout(dropout), nn.Linear(64, 1)\n",
        "        )\n",
        "        self.head_1y = nn.Sequential(\n",
        "            nn.Linear(bilstm_output_size, 64), nn.ReLU(), nn.Dropout(dropout), nn.Linear(64, 1)\n",
        "        )\n",
        "        self.head_3y = nn.Sequential(\n",
        "            nn.Linear(bilstm_output_size, 64), nn.ReLU(), nn.Dropout(dropout), nn.Linear(64, 1)\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = x.permute(0, 2, 1)\n",
        "        x = self.cnn(x)\n",
        "        x = x.permute(0, 2, 1)\n",
        "        \n",
        "        x, _ = self.bilstm(x)\n",
        "        \n",
        "        x = self.pos_encoder(x)\n",
        "        x = self.transformer(x)\n",
        "        \n",
        "        x = x.permute(0, 2, 1)\n",
        "        x = self.feature_pool(x)\n",
        "        x = x.squeeze(-1)\n",
        "        \n",
        "        return self.head_3m(x), self.head_1y(x), self.head_3y(x)\n",
        "\n",
        "print(\"HybridForecaster model defined ‚úÖ\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_one_epoch(model, dataloader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    n_batches = 0\n",
        "    \n",
        "    for X, Y in dataloader:\n",
        "        X = X.to(device)\n",
        "        Y = Y.to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        pred_3m, pred_1y, pred_3y = model(X)\n",
        "        \n",
        "        loss_3m = criterion(pred_3m.squeeze(), Y[:, 0])\n",
        "        loss_1y = criterion(pred_1y.squeeze(), Y[:, 1])\n",
        "        loss_3y = criterion(pred_3y.squeeze(), Y[:, 2])\n",
        "        \n",
        "        loss = loss_3m + loss_1y + loss_3y\n",
        "        \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        total_loss += loss.item()\n",
        "        n_batches += 1\n",
        "    \n",
        "    return total_loss / n_batches\n",
        "\n",
        "\n",
        "def train_stock(ticker, processed_dir, models_dir, device):\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Training model for: {ticker}\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    try:\n",
        "        dataset = StockDataset(\n",
        "            data_dir=processed_dir,\n",
        "            seq_length=SEQ_LENGTH,\n",
        "            tickers=[ticker],\n",
        "            train=True,\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error loading {ticker}: {e}\")\n",
        "        return None\n",
        "    \n",
        "    if len(dataset) == 0:\n",
        "        print(f\"‚ùå No samples for {ticker}, skipping...\")\n",
        "        return None\n",
        "    \n",
        "    print(f\"Dataset size: {len(dataset)} samples\")\n",
        "    \n",
        "    dataloader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=True,\n",
        "        num_workers=2,\n",
        "        pin_memory=True,\n",
        "    )\n",
        "    \n",
        "    sample_x, _ = dataset[0]\n",
        "    input_size = sample_x.shape[1]\n",
        "    \n",
        "    model = HybridForecaster(\n",
        "        input_size=input_size,\n",
        "        seq_length=SEQ_LENGTH,\n",
        "    ).to(device)\n",
        "    \n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, mode='min', factor=0.5, patience=5\n",
        "    )\n",
        "    \n",
        "    best_loss = float('inf')\n",
        "    \n",
        "    for epoch in tqdm(range(EPOCHS), desc=f\"Training {ticker}\"):\n",
        "        avg_loss = train_one_epoch(model, dataloader, optimizer, criterion, device)\n",
        "        scheduler.step(avg_loss)\n",
        "        \n",
        "        if avg_loss < best_loss:\n",
        "            best_loss = avg_loss\n",
        "        \n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(f\"  Epoch {epoch + 1}/{EPOCHS} | Loss: {avg_loss:.6f}\")\n",
        "    \n",
        "    model_path = models_dir / f\"{ticker}_model.pt\"\n",
        "    torch.save({\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'scaler_params': dataset.get_scaler_params(),\n",
        "        'best_loss': best_loss,\n",
        "        'epochs': EPOCHS,\n",
        "        'ticker': ticker,\n",
        "    }, model_path)\n",
        "    \n",
        "    print(f\"‚úÖ Model saved: {model_path}\")\n",
        "    print(f\"   Best loss: {best_loss:.6f}\")\n",
        "    \n",
        "    return best_loss\n",
        "\n",
        "print(\"Training functions defined ‚úÖ\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "csv_files = sorted(PROCESSED_DIR.glob(\"*.csv\"))\n",
        "tickers = [f.stem for f in csv_files]\n",
        "\n",
        "print(f\"Found {len(tickers)} stocks to train:\")\n",
        "print(\", \".join(tickers))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"\\n{'#'*60}\")\n",
        "print(f\"STARTING TRAINING FOR ALL {len(tickers)} STOCKS\")\n",
        "print(f\"{'#'*60}\")\n",
        "\n",
        "results = {}\n",
        "\n",
        "for i, ticker in enumerate(tickers):\n",
        "    print(f\"\\n[{i+1}/{len(tickers)}] Processing {ticker}...\")\n",
        "    \n",
        "    loss = train_stock(ticker, PROCESSED_DIR, MODELS_DIR, device)\n",
        "    \n",
        "    if loss is not None:\n",
        "        results[ticker] = loss\n",
        "    \n",
        "    torch.cuda.empty_cache()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"\\n{'#'*60}\")\n",
        "print(f\"TRAINING COMPLETE!\")\n",
        "print(f\"{'#'*60}\")\n",
        "\n",
        "print(f\"\\nSuccessfully trained: {len(results)}/{len(tickers)} models\")\n",
        "\n",
        "if results:\n",
        "    print(f\"\\nResults summary:\")\n",
        "    print(f\"{'-'*40}\")\n",
        "    sorted_results = sorted(results.items(), key=lambda x: x[1])\n",
        "    for ticker, loss in sorted_results:\n",
        "        print(f\"  {ticker:15} | Loss: {loss:.6f}\")\n",
        "    \n",
        "    print(f\"\\nBest performing: {sorted_results[0][0]} (loss: {sorted_results[0][1]:.6f})\")\n",
        "    print(f\"Worst performing: {sorted_results[-1][0]} (loss: {sorted_results[-1][1]:.6f})\")\n",
        "\n",
        "print(f\"\\nModels saved to: {MODELS_DIR}\")\n",
        "print(f\"Total models: {len(list(MODELS_DIR.glob('*.pt')))}\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
